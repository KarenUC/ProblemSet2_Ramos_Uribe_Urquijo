# summary(train_hogares)
# Modelo 1
model1_ols<- lm(Ingtotugarr ~ Dominio + viviendapropia
+ Nper + total_female + female_jh + num_ocu +
edad_jh+ edad_jh2 + menores + max_educ_jh
+ jh_ocup + prod_finan_jh+num_afsalud, data= train_hogares
)
summary(model1_ols)
# Predicciones de entrenamiento
# ==============================================================================
predicciones_ols <- predict(model1_ols, newdata = train_hogares)
summary(predicciones_ols)
# MAE de entrenamiento
# ==============================================================================
mae_ols <- mean(abs((predicciones_ols - train_hogares$Ingtotugarr)))
paste("Error (mae) de ols:", mae_ols)
##########################################################################################################################################################################
p_load(faraway)
# GrÃ¡ficos y tratamiento de datos
# ==============================================================================
p_load(tidyverse)
p_load(skimr)
p_load(DataExplorer)
p_load(scales)
p_load(corrr)
# Modelado
# ==============================================================================
p_load(glmnet)
p_load(pls)
# Matrices de entrenamiento y test
# ==============================================================================
x_train <- model.matrix(Ingtotugarr~Nper+Npersug+P5000+P5010
+viviendapropia+total_female+female_jh+num_ocu+edad_jh+menores
+max_educ_jh+jh_ocup+num_afsalud+prod_finan_jh,data = train_hogares)[, -1]
y_train <- train_hogares$Ingtotugarr
#### ---- Ridge -----######
# EvoluciÃ³n del error en funciÃ³n de lambda
# ==============================================================================
set.seed(123)
cv_error_ridge <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 0,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
cv_error_ridge
plot(cv_error_ridge)
# Mayor valor de lambda con el que el test-error no se aleja mÃ¡s de 1sd del mÃ­nimo.
paste("Mejor valor de lambda encontrado + 1 desviaciÃ³n estÃ¡ndar:", cv_error_ridge$lambda.1se)
# Mejor modelo lambda Ã³ptimo + 1sd
modelo_ridge <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 0,
lambda      = cv_error_ridge$lambda.1se,
standardize = TRUE
)
modelo_ridge
# Coeficientes del modelo
# ==============================================================================
df_coeficientes_ridge <- coef(modelo_ridge) %>%
as.matrix() %>%
as_tibble(rownames = "predictor") %>%
rename(coeficiente = s0)
# Predicciones de entrenamiento
# ==============================================================================
predicciones_train_ridge <- predict(modelo_ridge, newx = x_train)
# MAE de entrenamiento
# ==============================================================================
mae_ridge <- mean(abs(predicciones_train_ridge - y_train))
paste("Error (mae) de ridge", mae_ridge)
#### ---- Lasso -----######
# CreaciÃ³n y entrenamiento del modelo
# ==============================================================================
# Para obtener un ajuste con regularizaciÃ³n Lasso se indica argumento alpha=1.
# Si no se especifica valor de lambda, se selecciona un rango automÃ¡tico.
modelo_lasso <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 1,
nlambda     = 100,
standardize = TRUE
)
# EvoluciÃ³n de los coeficientes en funciÃ³n de lambda
# ==============================================================================
regularizacion_lasso <- modelo_lasso$beta %>%
as.matrix() %>%
t() %>%
as_tibble() %>%
mutate(lambda = modelo_lasso$lambda)
regularizacion_lasso <- regularizacion_lasso %>%
pivot_longer(
cols = !lambda,
names_to = "predictor",
values_to = "coeficientes"
)
regularizacion_lasso %>%
ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
geom_line() +
scale_x_log10(
breaks = trans_breaks("log10", function(x) 10^x),
labels = trans_format("log10", math_format(10^.x))
) +
labs(title = "Coeficientes del modelo en funciÃ³n de la regularizaciÃ³n") +
theme_bw() +
theme(legend.position = "none")
# EvoluciÃ³n del error en funciÃ³n de lambda
# ==============================================================================
set.seed(123)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error_lasso)
# Mayor valor de lambda con el que el test-error no se aleja mÃ¡s de 1sd del mÃ­nimo.
paste("Mejor valor de lambda encontrado + 1 desviaciÃ³n estÃ¡ndar:", cv_error_lasso$lambda.1se)
# Mejor modelo lambda Ã³ptimo + 1sd
# ==============================================================================
modelo_lasso <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 1,
lambda      = cv_error_lasso$lambda.1se,
standardize = TRUE
)
# Coeficientes del modelo
# ==============================================================================
df_coeficientes_lasso <- coef(modelo_lasso) %>%
as.matrix() %>%
as_tibble(rownames = "predictor") %>%
rename(coeficiente = s0)
df_coeficientes_lasso %>%
filter(
predictor != "(Intercept)",
coeficiente != 0
)
# Predicciones de entrenamiento
# ==============================================================================
predicciones_train_lasso <- predict(modelo_lasso, newx = x_train)
# MAE de entrenamiento
# ==============================================================================
mae_lasso <- mean(abs(predicciones_train_lasso - y_train))
print(paste("Error (mae) de lasso", mae_lasso))
### ----- K-fold ValidaciÃ³n cruzada ------#####
set.seed(123)
model1_kfold <- train(log_Ingtotugarr~ viviendapropia + Nper + total_female + female_jh + num_ocu +
edad_jh + menores + max_educ_jh + jh_ocup + num_afsalud + prod_finan_jh,
data = train_hogares, trControl = trainControl(method = "cv", number = 5),
method = "lm")
mae_kfold1<-model1_kfold$results[,4]
mae_kfold1
### modelo 2 tiene edad al cuadrado
model2_kfold <- train(log_Ingtotugarr~ viviendapropia + Nper + total_female + female_jh + num_ocu +
edad_jh + edad_jh2 + menores + max_educ_jh + jh_ocup + num_afsalud + prod_finan_jh,
data = train_hogares, trControl = trainControl(method = "cv", number = 5),
method = "lm")
mae_kfold2<-model2_kfold$results[,4]
mae_kfold2
mae_modelos<-rbind(mae_kfold1,mae_kfold2,mae_lasso, mae_ridge, mae_ols)
#El mejor modelo es lasso dado que es el que tiene menor MSE
# PredicciÃ³n modelo lasso con base test
X<-model.matrix(~Nper+Npersug+P5000+P5010
+viviendapropia+total_female+female_jh+num_ocu+edad_jh+menores
+max_educ_jh+jh_ocup+num_afsalud+prod_finan_jh, test_hogares)
X<-X[,-1]
test_hogares$Ingreso_predicho_final<-predict(modelo_lasso,newx = X)
#Prediccion final Modelo Clasificaci?n
summary(test_hogares$Ingreso_predicho_final)
hist(test_hogares$Ingreso_predicho_final)
test_hogares$Ing_Pred_test_hogares<-ifelse(test_hogares$Ingreso_predicho_final<=test_hogares$Lp,"Si","No")
# ########## Box cox para Linea de pobreza
# z<-as.numeric(train_hogares$Lp)
# lambda_lp<-boxcox(z, objective.name = "Log-Likelihood", optimize = T)$lambda
# #Transformamos la variable
# train_hogares$lp_boxcox<-boxcoxTransform(z, lambda_lp)
#
# summary(train_hogares$Lp)
# summary(train_hogares$lp_boxcox)
# a<-cbind(train_hogares$Lp,train_hogares$lp_boxcox)
##########Archivo de predicciones
id_test_hogares<-test_hogares$id
Pobre_Pred_test_hogares<-test_hogares$Pobre_predicho_final
Ing_Pred_test_hogares<-test_hogares$Ing_Pred_test_hogares
DB_test_hog<-data_frame(id_test_hogares,Pobre_Pred_test_hogares,Ing_Pred_test_hogares)
write.csv(DB_test_hog, file = "predictions.csv")
########
summary(DB_test_hog$Pobre_Pred_test_hogares)
summary(DB_test_hog$Pobre_Pred_test_hogares)
summary(DB_test_hog$Ing_Pred_test_hogares)
DB_test_hog$Ing_Pred_test_hogares -> as.factor(DB_test_hog$Ing_Pred_test_hogares)
DB_test_hog$Ing_Pred_test_hogares -> factor(DB_test_hog$Ing_Pred_test_hogares)
DB_test_hog$Ing_Pred_test_hogares -> as.factor(DB_test_hog$Ing_Pred_test_hogares)
table(DB_test_hog$Ing_Pred_test_hogares)
prop.table(DB_test_hog$Ing_Pred_test_hogares)
prop.table(DB_test_hog$Pobre_Pred_test_hogares)
w <- prop.table(DB_test_hog$Pobre_Pred_test_hogares)
addmargins(w)
View(train_hogares)
stargazer(train_hogares[c("edad_jh")], type = "latex")
stargazer(train_hogares[c("edad_jh")], type = "latex")
stargazer(train_hogares[c("Nper", "Ingtotugarr", "total_female", "edad_jh")], type = "latex")
class(train_hogares$edad_jh)
sum(train_hogares$edad_jh)
summary(train_hogares$edad_jh)
stargazer(train_hogares[c("Nper", "Ingtotugarr", "total_female", "edad_jh")], type = "text")
stargazer(train_hogares[c("Nper", "edad_jh")], type = "text")
stargazer(train_hogares[c("Nper", "Ingtotugarr", "total_female")], type = "text")
average(train_hogares$edad_jh)
averages(train_hogares$edad_jh)
summary(train_hogares$edad_jh)
# Paula Ramos, Karen Uribe y Juan D. Urquijo
# update: 07-07-2022
###----------------- Project Set 2----------###
##### ---Limpiar Ambiente --- ######
rm(list = ls())
##### ---Cargar paquetes --- ######
require(pacman)
# usar la funci?n p_load de pacman para instalar/llamar las librer?as de la clase
p_load(rio) # Librer?a para importar datos
p_load(tidyverse) # Librer?a para limpiar datos
p_load(e1071) # Tiene la funci?n para calcular skewness
p_load(tidymodels) # Modelos ML
p_load(ggplot2) # Librer?a para visualizar datos
p_load(scales) # Formato de los ejes en las gr?ficas
p_load(ggpubr) # Combinar gr?ficas
p_load(skimr, # summary data
caret, # Classification And REgression Training
rvest,
stringr,
dplyr,
robotstxt
)
#########################################################################################
#Cargar los datos
#test_hogares<-import("https://github.com/KarenUC/ProblemSet2_Ramos_Uribe_Urquijo/tree/main/data/test_hogares.Rds")
setwd("/Users/jdaviduu96/Documents/MECA 2022/Big Data y Machine Learning 2022-13/Problem set 2/ProblemSet2_Ramos_Uribe_Urquijo/")
#setwd("C:/Users/kurib/OneDrive - Universidad de los Andes/Documentos/MECA/Github/ProblemSet2_Ramos_Uribe_Urquijo/")
#setwd("C:/Users/pau_9/Documents/GitHub/ProblemSet2_Ramos_Uribe_Urquijo/")
unzip("dataPS2RDS.zip",  list =  T)
train_hogares<-readRDS("data/train_hogares.Rds")
train_personas<-readRDS("data/train_personas.Rds")
test_hogares<-readRDS("data/test_hogares.Rds")
test_personas<-readRDS("data/test_personas.Rds")
#########################################################################################
train<-left_join(train_hogares,train_personas, by="id")
test<-merge(test_hogares,test_personas, by="id")
# Utilizando el diccionario, identificamos variables categoricas para volverlas a tipo factor
summary(train)
train <- train %>%
mutate_at(.vars = c("Clase.x", "Dominio.x","P5090", "Pobre", "Indigente",
"Depto.x", "Clase.y", "Dominio.y", "Estrato1", "P6020",
"P6050", "P6090", "P6100", "P6210", "P6240", "Oficio",
"P6430", "P6510", "P6510s2", "P6545", "P6545s2", "P6580",
"P6580s2", "P6585s1", "P6585s1a2", "P6585s2", "P6585s2a2",
"P6585s3", "P6585s3a2", "P6585s4", "P6585s4a2", "P6590",
"P6600", "P6610", "P6620", "P6630s1", "P6630s4","P6630s2",
"P6630s3","P6870", "P6920", "P7040", "P7050", "P7090", "P7110",
"P7120", "P7140s1","P7140s2","P7150","P7160","P7310","P7150",
"P7350","P7422","P7472","P7495","P7500s1","P7500s2","P7500s3",
"P7505","P7510s1","P7510s2","P7510s3", "P7510s5","P7510s6","P7510s7",
"Pet", "Oc", "Des", "Ina", "Depto.y" ),.funs = factor)
test <- test %>%
mutate_at(.vars = c("Clase.x", "Dominio.x","P5090","Depto.x", "Clase.y",
"Dominio.y", "P6020","P6050", "P6090", "P6100",
"P6210", "P6240", "Oficio","P6430", "P6510",
"P6545", "P6580","P6585s1", "P6585s2", "P6585s3",
"P6585s4", "P6590","P6600", "P6610", "P6620", "P6630s1",
"P6630s4","P6630s2","P6630s3","P6870", "P6920", "P7040",
"P7050", "P7090", "P7110","P7120", "P7150","P7160",
"P7310","P7150","P7350","P7422","P7472","P7495",
"P7500s2","P7500s3","P7505","P7510s1","P7510s2",
"P7510s3", "P7510s5","P7510s6","P7510s7",
"Pet", "Oc", "Des", "Ina", "Depto.y" ),.funs = factor)
summary(train_personas$P7510s5)
summary(train$P7510s5)
### --- 2. a) Data Cleaning --- ###
### --- Missing Values
# Sacar cantidad de NAs por variable
cantidad_na <- sapply(train, function(x) sum(is.na(x)))
cantidad_na <- data.frame(cantidad_na)
cantidad_na<- data.frame(cantidad_na)%>%
rownames_to_column("variable")
cantidad_na$porcentaje_na <- cantidad_na$cantidad_na/nrow(train)
# Eliminamos variables que no aportan
filtro <- cantidad_na$porcentaje_na > 0.85
variables_eliminar <- cantidad_na$variable[filtro]
train <- train %>%
select(-variables_eliminar)
####################################################################################################
#QuÃ© variables estÃ¡n en la base train y test
x <- ls(train)
y <- ls(test)
Coincidencias <- list()
for(i in 1:length(y)){
for(j in 1:length(x)){
if (y[i]==x[j]){
Coincidencias <- append(Coincidencias, y[i])
}
}
}
Coincidencias
####################################################################################################
################################# Limpieza de Test #############################################
### --- 2. a) Data Cleaning --- ###
### --- Missing Values
# Sacar cantidad de NAs por variable
cantidad_na_test <- sapply(test, function(x) sum(is.na(x)))
cantidad_na_test <- data.frame(cantidad_na_test)
cantidad_na_test<- data.frame(cantidad_na_test)%>%
rownames_to_column("variable")
cantidad_na_test$porcentaje_na <- cantidad_na_test$cantidad_na/nrow(train)
# Eliminamos variables que no aportan
filtro <- cantidad_na_test$porcentaje_na > 0.85
variables_eliminar_test <- cantidad_na_test$variable[filtro]
test <- test %>%
select(-variables_eliminar_test)
### ----- Escoger variables que nos sirven para hacer el modelo -----####
#Caracteristicas del jefe del hogar (P6050- OpciÃ³n 1 es jefe del hogar - parentesco con jefe del hogar)
#genero_jef(P6020), ocupado (Oc), educaciÃ³n(P6210, P6210s1),desocupado (Des),
#ingreso (Ingtot)
#CaracterÃ­sticas de Individuo:
#Edad(P6040), educaciÃ³n(P6210, P6210s1), salud(P6090), ahorro(P7510s5), genero(P6020)
#desocupado (Des), ingreso (Ingtot)
#Caracteristicas del hogar:
#Vivienda propia (P5090), total personas en el hogar (), choques a salud(P6240, opciÃ³n 5),
#choques ,ingreso (Ingtotugarr)
female_test<-ifelse(test$P6020==2,1,0)
jh_test<-ifelse(test$P6050==1,1,0)
id<-test$id
DB_test<-data_frame(id,female_test,jh_test)
DB_test$female_jh<-ifelse(DB_test$jh_test==1,DB_test$female_test,0)
DB_test$ocu<-ifelse(is.na(test$Oc),0,1)
DB_test$edad<-test$P6040
DB_test$edad_jh<-ifelse(DB_test$jh_test==1,test$P6040,0)
DB_test$menores<-ifelse(DB_test$edad<18,1,0)
DB_test$max_educ_jh <-ifelse(DB_test$jh_test==1,test$P6210s1,0)
DB_test$jh_ocup <-ifelse(DB_test$jh_test==1,DB_test$ocu,0)
DB_test$afiliado<-ifelse(test$P6090!=1 | is.na(test$P6090),0,1)
DB_test$prod_finan_jh<-(ifelse(test$P7510s5!=1 | is.na(test$P7510s5),0,1))
DB_test$prod_finan_jh<-(ifelse(DB_test$jh_test==1 & DB_test$prod_finan_jh==1,1,0))
DB_2_test<-DB_test %>% group_by(id) %>% summarise(total_female = sum(female_test),
female_jh = sum(female_jh),
num_ocu = sum(ocu),
edad_jh = sum(edad_jh),
menores= sum(menores),
max_educ_jh= sum(max_educ_jh),
jh_ocup= sum(jh_ocup),
num_afsalud = sum(afiliado),
prod_finan_jh=sum(prod_finan_jh)
)
test_hogares <-test_hogares %>% left_join(DB_2_test,by="id")
test_hogares$viviendapropia <-as.factor(ifelse (test_hogares$P5090==1 | test_hogares$P5090==2,1,0))
sum(is.na(test_hogares$max_educ_jh))
#Eliminar NA en max_edu_jh
test_hogares <- test_hogares[!is.na(test_hogares$max_educ_jh),]
#Base Final TEST
########################################################################################################################################
### ----- Escoger variables que nos sirven para hacer el modelo -----####
#Decidimos hacerlo v?a dos partes: Caracteristicas del jefe del hogar y caracteristicas propias del hogar
#Caracteristicas del jefe del hogar (P6050- OpciÃ³n 1 es jefe del hogar - parentesco con jefe del hogar)
#genero_jef(P6020), ocupado (Oc), educaciÃ³n(P6210, P6210s1),desocupado (Des),
#ingreso (Ingtot)
#CaracterÃ­sticas de Individuo:
#Edad(P6040), educaciÃ³n(P6210, P6210s1), salud(P6090), ahorro(P7510s5), genero(P6020)
#desocupado (Des), ingreso (Ingtot)
#Caracteristicas del hogar:
#Vivienda propia (P5090), total personas en el hogar (), choques a salud(P6240, opciÃ³n 5),
#choques ,ingreso (Ingtotugarr)
female<-ifelse(train$P6020==2,1,0)
jh<-ifelse(train$P6050==1,1,0)
id<-train$id
DB<-data_frame(id,female,jh)
DB$female_jh<-ifelse(DB$jh==1,DB$female,0)
DB$ocu<-ifelse(is.na(train$Oc),0,1)
DB$edad<-train$P6040
DB$edad_jh<-ifelse(DB$jh==1,train$P6040,0)
DB$menores<-ifelse(DB$edad<18,1,0)
DB$max_educ_jh <-ifelse(DB$jh==1,train$P6210s1,0)
DB$jh_ocup <-ifelse(DB$jh==1,DB$ocu,0)
DB$afiliado<-ifelse(train$P6090!=1 | is.na(train$P6090),0,1)
DB$prod_finan_jh<-(ifelse(train$P7510s5!=1 | is.na(train$P7510s5),0,1))
DB$prod_finan_jh<-(ifelse(DB$jh==1 & DB$prod_finan_jh==1,1,0))
DB$Estrato<-ifelse(DB$jh==1,train$Estrato1,0)
DB$Ingtot<- train$Ingtot
DB$Ingtot_jh<-ifelse(DB$jh==1,DB$Ingtot, 0)
DB_2<-DB %>% group_by(id) %>% summarise(total_female = sum(female),
female_jh = sum(female_jh),
num_ocu = sum(ocu),
edad_jh = sum(edad_jh),
menores= sum(menores),
Ingtot_jh = sum(Ingtot_jh),
max_educ_jh= sum( max_educ_jh),
jh_ocup= sum(jh_ocup),
num_afsalud = sum(afiliado),
jh_estrato=sum(Estrato),
prod_finan_jh=sum(prod_finan_jh)
)
train_hogares <-train_hogares %>% left_join(DB_2,by="id")
#Estadisticas descriptivas
train_hogares <- train_hogares %>%
mutate_at(.vars = c("Clase", "Dominio","P5090", "Pobre", "Indigente",
"Depto", "female_jh", "jh_ocup", "jh_estrato" ),.funs = factor)
library(stargazer)
library(skimr)
skim(train_hogares)
#VerificaciÃ³n de missing values
sum(is.na(train_hogares$Ingtot_jh))
sum(is.na(train_hogares$max_educ_jh))
#ImputaciÃ³n de datos
train_hogares = train_hogares %>%
mutate(Ingtot_jh = ifelse(is.na(Ingtot_jh),
yes = Ingtotugarr,
no = Ingtot_jh))
#Eliminar NA en max_edu_jh
train_hogares <- train_hogares[!is.na(train_hogares$max_educ_jh),]
##Creacion de variable Vivienda Propia
train_hogares$viviendapropia <-as.factor(ifelse (train_hogares$P5090==1 | train_hogares$P5090==2,1,0))
stargazer(train_hogares[c("Nper", "Ingtotugarr", "total_female", "num_ocu")], type = "text")
female<-ifelse(train$P6020==2,1,0)
jh<-ifelse(train$P6050==1,1,0)
id<-train$id
DB<-data_frame(id,female,jh)
DB$female_jh<-ifelse(DB$jh==1,DB$female,0)
DB$ocu<-ifelse(is.na(train$Oc),0,1)
DB$edad<-train$P6040
DB$edad_jh<-ifelse(DB$jh==1,train$P6040,0)
DB$menores<-ifelse(DB$edad<18,1,0)
DB$max_educ_jh <-ifelse(DB$jh==1,train$P6210s1,0)
DB$jh_ocup <-ifelse(DB$jh==1,DB$ocu,0)
DB$afiliado<-ifelse(train$P6090!=1 | is.na(train$P6090),0,1)
DB$prod_finan_jh<-(ifelse(train$P7510s5!=1 | is.na(train$P7510s5),0,1))
DB$prod_finan_jh<-(ifelse(DB$jh==1 & DB$prod_finan_jh==1,1,0))
DB$Estrato<-ifelse(DB$jh==1,train$Estrato1,0)
DB$Ingtot<- train$Ingtot
DB$Ingtot_jh<-ifelse(DB$jh==1,DB$Ingtot, 0)
DB_2<-DB %>% group_by(id) %>% summarise(total_female = sum(female),
female_jh = sum(female_jh),
num_ocu = sum(ocu),
edad_jh = sum(edad_jh),
menores= sum(menores),
Ingtot_jh = sum(Ingtot_jh),
max_educ_jh= sum( max_educ_jh),
jh_ocup= sum(jh_ocup),
num_afsalud = sum(afiliado),
jh_estrato=sum(Estrato),
prod_finan_jh=sum(prod_finan_jh)
)
train_hogares <-train_hogares %>% left_join(DB_2,by="id")
train_hogares <- train_hogares %>%
mutate_at(.vars = c("Clase", "Dominio","P5090", "Pobre", "Indigente",
"Depto", "female_jh", "jh_ocup", "jh_estrato" ),.funs = factor)
#VerificaciÃ³n de missing values
sum(is.na(train_hogares$Ingtot_jh))
train_hogares <-train_hogares %>% left_join(DB_2,by="id")
train_hogares <- train_hogares %>%
mutate_at(.vars = c("Clase", "Dominio","P5090", "Pobre", "Indigente",
"Depto", "female_jh", "jh_ocup", "jh_estrato" ),.funs = factor)
library(stargazer)
#VerificaciÃ³n de missing values
sum(is.na(train_hogares$Ingtot_jh))
sum(is.na(train_hogares$max_educ_jh))
#Eliminar NA en max_edu_jh
train_hogares <- train_hogares[!is.na(train_hogares$max_educ_jh),]
##Creacion de variable Vivienda Propia
train_hogares$viviendapropia <-as.factor(ifelse (train_hogares$P5090==1 | train_hogares$P5090==2,1,0))
stargazer(train_hogares[c("Nper", "Ingtotugarr", "total_female", "num_ocu")], type = "text")
stargazer(train_hogares[c("Nper", "Ingtotugarr", "total_female", "num_ocu")], type = "latex")
stargazer(train_hogares[c("menores", "Ingtot_jh", "max_educ_jh", "num_afsalud")], type = "text")
stargazer(as.data.frame(train_hogares[c("Nper", "Ingtotugarr", "total_female", "num_ocu")], type = "text") )
stargazer(as.data.frame(train_hogares[c("Nper", "Ingtotugarr", "total_female", "num_ocu")], type = "text"))
stargazer(as.data.frame(train_hogares[c("Nper", "total_female", "num_ocu")], type = "text"))
#VerificaciÃ³n de missing values
sum(is.na(train_hogares$Ingtot_jh))
sum(is.na(train_hogares$max_educ_jh))
stargazer(as.data.frame(train_hogares[c("Nper", "Ingtotugarr", "total_female", "num_ocu")], type = "text"))
stargazer(as.data.frame(train_hogares[c("menores", "Ingtot_jh", "max_educ_jh", "num_afsalud")], type = "text"))
stargazer(as.data.frame(train_hogares[c("edad_jh")], type = "text"))
summary(train_hogares$Pobre)
prop.table(train_hogares$Pobre)
prop.table(as.numeric(train_hogares$Pobre))
summary(train_hogares$Pobre)
total_graphs <- ggarrange(pobre_bar, ing_graph)
pobre_bar <- ggplot(data = train_hogares, aes(x = Pobre, fill=Pobre)) +
geom_bar() +
labs (subtitle="Base de Entrenamiento",
x= "Pobre = 1", y = "N?mero de Pobres")
total_graphs <- ggarrange(pobre_bar, ing_graph)
ing_graph <- ggarrange(box_plot, box_plot2) ## Para incluir en el an?lisis
box_plot <- ggplot(data=train_hogares , mapping = aes(as.factor(Pobre) , Ingtotugarr)) +
geom_boxplot()
box_plot <- box_plot +
scale_fill_grey() + theme_classic()+
labs(x= "Pobres", y ="Ingresos Totales Hogar")
train_hogares$log_ingh <- log(train_hogares$Ingtotugarr+1)
box_plot2 <- ggplot(data=train_hogares , mapping = aes(as.factor(Pobre) , log_ingh)) +
geom_boxplot()
box_plot2 <- box_plot2 +
scale_fill_grey() + theme_classic()+
labs(x= "Pobres", y =" Logaritmo Ingresos Totales Hogar")
ing_graph <- ggarrange(box_plot, box_plot2) ## Para incluir en el an?lisis
pobre_bar <- ggplot(data = train_hogares, aes(x = Pobre, fill=Pobre)) +
geom_bar() +
labs (subtitle="Base de Entrenamiento",
x= "Pobre = 1", y = "N?mero de Pobres")
total_graphs <- ggarrange(pobre_bar, ing_graph)
total_graphs
